---
title: PyTorch
category: ai-ml/frameworks
tags: [AI, æ·±åº¦å­¦ä¹ , PyTorch, Python, æ¡†æ¶]
created: 2025-12-27
updated: 2025-12-27
mastery: 2
difficulty: 3
review_dates:
  - 2025-12-28
  - 2025-12-30
  - 2026-01-03
  - 2026-01-10
  - 2026-01-24
  - 2026-03-24
status: learning
related:
  - ai-ml/basics/deep-learning-basics.md
  - ai-ml/tts/indextts2.md
---

# PyTorch

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯
- **ä¸»é¢˜**: PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
- **åˆ†ç±»**: AI/ML - æ¡†æ¶
- **æ ‡ç­¾**: #AI #æ·±åº¦å­¦ä¹  #PyTorch #Python #æ¡†æ¶
- **åˆ›å»ºæ—¥æœŸ**: 2025-12-27
- **æŒæ¡ç¨‹åº¦**: â­â­â­ (ç³»ç»Ÿå­¦ä¹ )
- **éš¾åº¦ç­‰çº§**: â­â­â­
- **å¼€å‘è€…**: Meta (Facebook) AI Research
- **é¦–æ¬¡å‘å¸ƒ**: 2016å¹´
- **è®¸å¯è¯**: BSD 3-Clause

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### å®šä¹‰
**PyTorch** æ˜¯ç”± Meta AI Research å¼€å‘çš„å¼€æºæœºå™¨å­¦ä¹ æ¡†æ¶ï¼ŒåŸºäº Torch åº“ï¼Œä½¿ç”¨ Python è¯­è¨€å®ç°ã€‚å®ƒæä¾›äº†ä¸¤ä¸ªé«˜çº§åŠŸèƒ½ï¼š
1. **å¼ é‡è®¡ç®—** - ç±»ä¼¼ NumPy çš„ GPU åŠ é€Ÿå¼ é‡è¿ç®—
2. **è‡ªåŠ¨å¾®åˆ†** - æ„å»ºç¥ç»ç½‘ç»œæ‰€éœ€çš„è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿ

### ä¸ TensorFlow å¯¹æ¯”

| ç‰¹æ€§ | PyTorch | TensorFlow |
|------|---------|------------|
| **ç¼–ç¨‹é£æ ¼** | åŠ¨æ€å›¾ï¼ŒPythonic | é™æ€å›¾ï¼Œè¾ƒä¸ºå¤æ‚ |
| **è°ƒè¯•** | æ˜“äºè°ƒè¯• (Python pdb) | è°ƒè¯•å›°éš¾ |
| **éƒ¨ç½²** | éœ€è¦ TorchScript | åŸç”Ÿæ”¯æŒ |
| **ç¤¾åŒº** | å­¦æœ¯ç•Œä¸»æµ | å·¥ä¸šç•Œä¸»æµ |
| **å­¦ä¹ æ›²çº¿** | å¹³ç¼“ | é™¡å³­ |

### PyTorch çš„æ ¸å¿ƒç»„ä»¶

```
PyTorch æ¶æ„
â”œâ”€â”€ torch           # æ ¸å¿ƒå¼ é‡åº“
â”œâ”€â”€ torch.autograd   # è‡ªåŠ¨å¾®åˆ†
â”œâ”€â”€ torch.nn        # ç¥ç»ç½‘ç»œæ¨¡å—
â”œâ”€â”€ torch.optim     # ä¼˜åŒ–ç®—æ³•
â”œâ”€â”€ torch.utils     # å·¥å…·å‡½æ•° (DataLoader ç­‰)
â””â”€â”€ torchvision     # è®¡ç®—æœºè§†è§‰å·¥å…·åŒ…
```

## ğŸ” è¯¦ç»†è§£é‡Š

### 1. å¼ é‡ (Tensor)

å¼ é‡æ˜¯ PyTorch çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œç±»ä¼¼äº NumPy çš„æ•°ç»„ï¼Œä½†å¯ä»¥åœ¨ GPU ä¸Šè¿è¡Œã€‚

```python
import torch

# åˆ›å»ºå¼ é‡
x = torch.tensor([1, 2, 3, 4])           # ä»åˆ—è¡¨åˆ›å»º
x = torch.zeros(3, 4)                     # å…¨é›¶å¼ é‡
x = torch.ones(2, 3)                      # å…¨ä¸€å¼ é‡
x = torch.randn(2, 3)                     # éšæœºæ­£æ€åˆ†å¸ƒ

# å¼ é‡è¿ç®—
y = x + 1                                  # åŠ æ³•
z = torch.matmul(x, y)                   # çŸ©é˜µä¹˜æ³•

# GPU è½¬ç§»
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
x = x.to(device)

# å¼ é‡å½¢çŠ¶æ“ä½œ
x = x.reshape(2, 6)                       # é‡å¡‘å½¢çŠ¶
x = x.permute(1, 0)                       # äº¤æ¢ç»´åº¦
x = x.squeeze()                            # å»é™¤ç»´åº¦ä¸º1çš„ç»´åº¦
```

### 2. è‡ªåŠ¨å¾®åˆ† (Autograd)

PyTorch çš„è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿç”¨äºè®¡ç®—æ¢¯åº¦ï¼š

```python
import torch

# å¯ç”¨æ¢¯åº¦è·Ÿè¸ª
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = torch.tensor([1.0, 1.0], requires_grad=False)

# å‰å‘è®¡ç®—
z = x * 2 + y

# åå‘ä¼ æ’­ (è®¡ç®—æ¢¯åº¦)
z.sum().backward()

print(x.grad)  # tensor([2., 2.])
```

### 3. ç¥ç»ç½‘ç»œæ¨¡å— (nn.Module)

`nn.Module` æ˜¯æ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å‹çš„åŸºç±»ï¼š

```python
import torch.nn as nn

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        # å®šä¹‰å±‚
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        # å®šä¹‰å‰å‘ä¼ æ’­
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

# åˆ›å»ºæ¨¡å‹
model = NeuralNetwork()
```

### 4. ä¼˜åŒ–å™¨ (Optimizer)

ä¼˜åŒ–å™¨ç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ï¼š

```python
import torch.optim as optim

# å¸¸ç”¨ä¼˜åŒ–å™¨
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

# è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
loss.backward()        # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
optimizer.step()       # æ›´æ–°å‚æ•°
```

### 5. æ•°æ®åŠ è½½ (DataLoader)

`DataLoader` æä¾›æ•°æ®æ‰¹å¤„ç†å’Œæ‰“ä¹±ï¼š

```python
from torch.utils.data import DataLoader, Dataset

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=32,
    shuffle=True,      # æ‰“ä¹±æ•°æ®
    num_workers=4,      # å¤šè¿›ç¨‹åŠ è½½
    pin_memory=True     # é”é¡µå†…å­˜ï¼ŒåŠ é€Ÿ GPU ä¼ è¾“
)

# è¿­ä»£æ•°æ®
for batch_idx, (data, target) in enumerate(train_loader):
    # è®­ç»ƒä»£ç 
    pass
```

## ğŸ’» ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹1: å®Œæ•´è®­ç»ƒæµç¨‹

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# 1. å®šä¹‰æ¨¡å‹
class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 2)

    def forward(self, x):
        return self.fc(x)

# 2. åˆå§‹åŒ–
model = SimpleNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 3. è®­ç»ƒå¾ªç¯
num_epochs = 10

for epoch in range(num_epochs):
    model.train()  # è®­ç»ƒæ¨¡å¼
    total_loss = 0

    for data, target in train_loader:
        data, target = data.to(device), target.to(device)

        # å‰å‘ä¼ æ’­
        output = model(data)
        loss = criterion(output, target)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}')

# 4. ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), 'model.pth')

# 5. åŠ è½½æ¨¡å‹
model.load_state_dict(torch.load('model.pth'))
```

### ç¤ºä¾‹2: CNN å›¾åƒåˆ†ç±»

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super(CNN, self).__init__()
        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        # æ± åŒ–å±‚
        self.pool = nn.MaxPool2d(2, 2)
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(64 * 8 * 8, 128)
        self.fc2 = nn.Linear(128, num_classes)
        # Dropout
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # å·ç§¯ -> ReLU -> æ± åŒ–
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        # å±•å¹³
        x = x.view(x.size(0), -1)
        # å…¨è¿æ¥ -> Dropout
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.fc2(x)
        return x
```

### ç¤ºä¾‹3: GPU åŠ é€Ÿä½¿ç”¨

```python
# æ£€æŸ¥ CUDA å¯ç”¨æ€§
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"GPU count: {torch.cuda.device_count()}")

# è®¾å¤‡ç®¡ç†
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# æ¨¡å‹å’Œæ•°æ®ç§»åˆ° GPU
model = model.to(device)
data = data.to(device)
target = target.to(device)
```

## ğŸ”— ç›¸å…³æ¦‚å¿µ

### å‰ç½®çŸ¥è¯†
- **[Python åŸºç¡€](https://github.com/)** - é¢å‘å¯¹è±¡ç¼–ç¨‹ (å»ºè®®æŒæ¡åº¦: â­â­â­â­)
- **[æ·±åº¦å­¦ä¹ åŸºç¡€](ai-ml/basics/deep-learning-basics.md)** - ç¥ç»ç½‘ç»œåŸç† (å»ºè®®æŒæ¡åº¦: â­â­â­)
- **[NumPy](https://numpy.org/)** - æ•°å€¼è®¡ç®— (å»ºè®®æŒæ¡åº¦: â­â­â­)

### ç›¸å…³æŠ€æœ¯
- **[TensorFlow](https://tensorflow.org/)** - Google çš„æ·±åº¦å­¦ä¹ æ¡†æ¶
- **[JAX](https://github.com/google/jax)** - Google çš„æ–°æ¡†æ¶
- **[Keras](https://keras.io/)** - é«˜å±‚ç¥ç»ç½‘ç»œ API

### åº”ç”¨åœºæ™¯
- **è®¡ç®—æœºè§†è§‰** - å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹
- **è‡ªç„¶è¯­è¨€å¤„ç†** - Transformerã€BERTã€GPT
- **è¯­éŸ³å¤„ç†** - ASRã€TTS (å¦‚ IndexTTS2)
- **å¼ºåŒ–å­¦ä¹ ** - DQNã€PPO

## ğŸ“š æ·±å…¥å­¦ä¹ èµ„æº

### ğŸ“– å®˜æ–¹æ–‡æ¡£
- **[PyTorch å®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/index.html)** - è‹±æ–‡æ–‡æ¡£
- **[PyTorch ä¸­æ–‡æ–‡æ¡£](https://pytorch.it-docs.cn/)** - ä¸­æ–‡ç¿»è¯‘
- **[PyTorch æ•™ç¨‹](https://pytorch.org/tutorials/)** - å®˜æ–¹æ•™ç¨‹

### ğŸ¥ è§†é¢‘æ•™ç¨‹
- **[2025æœ€æ–°PyTorchå…¥é—¨](https://www.bilibili.com/video/BV1TFtwzuESX/)** - Bç«™ï¼Œ100é›†å®Œæ•´æ•™ç¨‹
- **[PyTorch å®æˆ˜](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)** - æ·±åº¦å­¦ä¹ æ¡†æ¶

### ğŸ“° æ¨èæ–‡ç« 
- **[PyTorch å…¥é—¨å…¨æŒ‡å—](https://blog.csdn.net/u013970991/article/details/156082998)** - CSDN
- **[PyTorch æœ€ä½³å®è·µ](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)** - æ€§èƒ½ä¼˜åŒ–

### ğŸ’» å®æˆ˜é¡¹ç›®
- **[MNIST æ‰‹å†™æ•°å­—è¯†åˆ«](https://pytorch.org/tutorials/beginner/basics/intro.html)** - å…¥é—¨ç»å…¸
- **[CIFAR-10 å›¾åƒåˆ†ç±»](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)** - è¿›é˜¶é¡¹ç›®
- **[Transfer Learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)** - è¿ç§»å­¦ä¹ 

## âš ï¸ å¸¸è§è¯¯åŒºå’Œé™·é˜±

### è¯¯åŒº1: å¿½ç•¥ device ç®¡ç†
- **é”™è¯¯**: å¼ é‡åœ¨ä¸åŒè®¾å¤‡ä¸Šè®¡ç®—
- **æ­£ç¡®**: ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡
```python
# æ­£ç¡®åšæ³•
model = model.to(device)
data = data.to(device)
target = target.to(device)
```

### è¯¯åŒº2: å¿˜è®° zero_grad
- **é”™è¯¯**: æ¢¯åº¦ç´¯ç§¯å¯¼è‡´è®­ç»ƒå¼‚å¸¸
- **æ­£ç¡®**: æ¯ä¸ª batch å¼€å§‹å‰æ¸…ç©ºæ¢¯åº¦
```python
optimizer.zero_grad()  # å¿…é¡»åœ¨ backward() å‰è°ƒç”¨
```

### è¯¯åŒº3: eval æ¨¡å¼å¿˜è®°åˆ‡æ¢
- **é”™è¯¯**: æµ‹è¯•æ—¶ä½¿ç”¨ Dropout å’Œ BatchNorm è®­ç»ƒæ¨¡å¼
- **æ­£ç¡®**: æµ‹è¯•æ—¶åˆ‡æ¢åˆ° eval æ¨¡å¼
```python
model.eval()  # æµ‹è¯•æ—¶
with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
    output = model(data)
model.train()  # è®­ç»ƒæ—¶è®°å¾—åˆ‡æ¢å›æ¥
```

### è¯¯åŒº4: Batch size è®¾ç½®ä¸å½“
- **é”™è¯¯**: Batch size å¤ªå¤§æˆ–å¤ªå°
- **å»ºè®®**: 32/64/128 æ ¹æ®æ˜¾å­˜è°ƒæ•´
- **æ˜¾å­˜ä¸è¶³**: å‡å° batch size æˆ–ä½¿ç”¨ gradient accumulation

### é™·é˜±: CUDA Out of Memory
**é—®é¢˜**: GPU æ˜¾å­˜ä¸è¶³
```python
# è§£å†³æ–¹æ¡ˆ
# 1. å‡å° batch size
train_loader = DataLoader(dataset, batch_size=16)  # ä» 32 å‡åˆ° 16

# 2. æ¸…ç©ºç¼“å­˜
torch.cuda.empty_cache()

# 3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
for i, (data, target) in enumerate(train_loader):
    output = model(data)
    loss = criterion(output, target)
    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

## ğŸ¯ å®è·µæŠ€å·§

### 1. æ¨¡å‹ä¿å­˜ä¸åŠ è½½
```python
# ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), 'model.pth')

# åŠ è½½æ¨¡å‹
model = ModelClass()
model.load_state_dict(torch.load('model.pth'))
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
```

### 2. æ£€æŸ¥ç‚¹ä¿å­˜ (Checkpoint)
```python
# ä¿å­˜è®­ç»ƒçŠ¶æ€
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}
torch.save(checkpoint, 'checkpoint.pth')

# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch'] + 1
```

### 3. å­¦ä¹ ç‡è°ƒåº¦
```python
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR

# é˜¶æ¢¯è¡°å‡
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

# ä½™å¼¦é€€ç«
scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)

# è®­ç»ƒä¸­ä½¿ç”¨
scheduler.step()
```

### 4. æ··åˆç²¾åº¦è®­ç»ƒ (åŠ é€Ÿ)
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in train_loader:
    data, target = data.to(device), target.to(device)

    with autocast():  # è‡ªåŠ¨æ··åˆç²¾åº¦
        output = model(data)
        loss = criterion(output, target)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

## ğŸ“ ä¸ªäººç¬”è®°

### ç†è§£è¦ç‚¹
- PyTorch ä½¿ç”¨åŠ¨æ€å›¾ï¼Œè°ƒè¯•æ–¹ä¾¿ï¼Œé€‚åˆç ”ç©¶
- å¼ é‡æ˜¯æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œç±»ä¼¼ NumPy ä½†æ”¯æŒ GPU
- nn.Module æ˜¯æ‰€æœ‰æ¨¡å‹çš„åŸºç±»
- è®°ä½è®­ç»ƒäº”æ­¥: å‰å‘ â†’ æŸå¤± â†’ æ¸…ç©ºæ¢¯åº¦ â†’ åå‘ â†’ æ›´æ–°

### å¸¸ç”¨ä»£ç ç‰‡æ®µ
```python
# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)

# æŸ¥çœ‹æ¨¡å‹å‚æ•°æ•°é‡
total_params = sum(p.numel() for p in model.parameters())

# åªæ›´æ–°éƒ¨åˆ†å‚æ•°
optimizer = optim.Adam([
    {'params': model.fc.parameters(), 'lr': 1e-3},
    {'params': model.conv.parameters(), 'lr': 1e-4}
])
```

### ç–‘é—®è®°å½•
- [ ] å¦‚ä½•å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ï¼Ÿ
- [ ] å¦‚ä½•è¿›è¡Œæ¨¡å‹é‡åŒ–å’Œéƒ¨ç½²ï¼Ÿ
- [ ] å¦‚ä½•ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Ÿ

---

**çŠ¶æ€**: âœ… ç³»ç»Ÿå­¦ä¹ å®Œæˆï¼Œå»ºè®®å®è·µå·©å›º

**ä¸‹ä¸€æ­¥**:
1. å®è·µ MNIST æ‰‹å†™æ•°å­—åˆ†ç±»
2. å­¦ä¹  Transfer Learning
3. å­¦ä¹  Transformer å’Œ BERT
4. ç»§ç»­å­¦ä¹  [IndexTTS2](ai-ml/tts/indextts2.md) çš„éƒ¨ç½²
